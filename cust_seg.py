# -*- coding: utf-8 -*-
"""Cust_Seg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Ou5-XAn6ZV4HB3cAaq5V-xqIuFXOhVh

**Import Libraries & Dataset**
"""

!pip install kneed

# Commented out IPython magic to ensure Python compatibility.
#Import libraries
#loading dataset
import pandas as pd
import numpy as np

#visualisation
import matplotlib.pyplot as plt
# %matplotlib inline
import plotly.express as px
import seaborn as sns
from kneed import KneeLocator

# data modeling
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import ward,dendrogram,linkage
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN
from sklearn.cluster import AgglomerativeClustering
from sklearn.mixture import GaussianMixture

# Model performance
from sklearn.preprocessing import  StandardScaler
from sklearn import metrics
from sklearn.metrics import silhouette_score
from tqdm import tqdm
from sklearn.metrics import calinski_harabasz_score
from sklearn.metrics import davies_bouldin_score
from sklearn.decomposition import PCA

#warnings
import warnings
warnings.simplefilter(action='ignore')

#read_data
data=pd.read_csv("/content/drive/MyDrive/Customer segmentation/Customer DataSet.csv")
#dataframe
df=pd.DataFrame(data)
df

"""**Analysis and Visualization**"""

#get some information about the data
df.info()

# Describe the data
df.describe().T

df.nunique()

"""**Missing value & duplicated raw**"""

def missing_check(df):
    total = df.isnull().sum().sort_values(ascending=False) # Total number of null values
    percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending=False) # Percentage of values that are null
    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) # Putting the above two together
    return missing_data

missing_check(df)

mask = df['CREDIT_LIMIT'].isnull()
df[mask]

mask2 = df['MINIMUM_PAYMENTS'].isnull()
data[mask2].describe().T

corr=df.corr()
fig = plt.figure(figsize=(6,8))
r = sns.heatmap(corr, cmap='Reds')
r.set_title("Correlation ")

"""**Based on the analysis:**
1.   Since the credit limit feature has only 0.01% of records with missing values (just 1 record), it is reasonable to drop that record.
2.  The Customer ID feature is unique for each customer and does not contribute to determining clusters, so it can be dropped as well.
3.   All values in the PRC_FULL_PAYMENT column are zero, which suggests it might not be informative for clustering.
4.   The feature MINIMUM_PAYMENTS exhibits varied values, making it unsuitable for imputation using univariate techniques.
5. MINIMUM_PAYMENTS is not highly correlated with other features.
6. The number of missing data is small compared to the total dataset.

*Conclusively, rows with missing values can be safely removed from the dataset.*








"""

# create a copy for the data transformation
df1=df.copy()
df1.dropna(inplace = True)  # drop null values
df1.reset_index(drop = True, inplace = True)

#customer ID, does not reflect any useful information about the data.
df1.drop('CUST_ID', axis = 1, inplace = True)  #drop customer id
df1.reset_index(drop = True, inplace = True)
df1

# Create a 4x4 grid of subplots with a specified size
fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(15, 15))

# Set the style of the plots to 'darkgrid' for a dark background with grid lines
sns.set_style('darkgrid')

# Loop through each subplot in the grid
for i in range(4):
    for j in range(4):
        # Plot the distribution of each numerical feature using seaborn's distplot
        sns.distplot(df1[df1.columns[4 * i + j]], ax=axs[i,j],color='green')

# Show the plot
plt.show()

"""Considering the data preprocessing, certain features exhibit significant skewness, necessitating further inspection to detect outliers."""

#counts the number of duplicated rows
df1.duplicated().sum()

"""**Noise data**"""

# This section iterates over each column in the DataFrame
i = 0
while i < len(df1.columns):
    try:
        # Create a new figure for each column's boxplot
        fig = plt.figure(figsize=(8, 3), dpi=80)

        # Create a boxplot for the current column
        sns.boxplot(x=df1.columns[i], data=df1 , color='red')

        # Move to the next column
        i += 1
    except:
        # If an error occurs (e.g., column type not compatible with boxplot), skip to the next column
        continue

# Display all the generated boxplots together
plt.show()

"""**Based on the plotted graphs:**

1. BALANCE: Outliers exceeding 17500 may indicate noise.
2. BALANCE_FREQUENCY: Occasional zeros, though not significant.
3. PURCHASES: Outliers surpassing 30000, with one around 50000 possibly noise.
4. ONEOFF_PURCHASES: Outliers beyond 33000.
5. INSTALLMENT_PURCHASES: A value over 20000 may be noise.
6. CASH_ADVANCE: Values exceeding 40000 are outliers.
7. PURCHASES_FREQUENCY: No outliers observed.
8. ONEOFF_PURCHASES_FREQUENCY: No noticeable outliers.
9. PURCHASES_INSTALLMENT_FREQUENCY: No outliers detected.
10. CASH_ADVANCE_FREQUENCY: A value above 14 seems outlier, albeit low chance of being noise.
11. CASH_ADVANCE_TRX: Majority of values near "0", requires analysis in relation to other features.
12. PURCHASES_TRX: Similar to CASH_ADVANCE_TRX, requires contextual analysis.
13. CREDIT_LIMIT: Samples at or above 25000 warrant inspection.
14. PAYMENTS: Several outliers above 45000.
15. MINIMUM_PAYMENTS: Values exceeding 35000 warrant careful review.
16. PRC_FULL_PAYMENT: No outliers or noise.
17. TENURE: Discrete values, requires further examination.

**Handling Noises**
"""

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 5), dpi=90)

# Left subplot: Boxplot
sns.boxplot(x=df1.BALANCE, showmeans=True, orient='h', color="orange", data=df1, ax=axes[0])
# Set the orientation to horizontal and show the mean ,Color the boxplot with violet color ,Data source is the 'BALANCE' column of DataFrame df1

# Right subplot: Histogram
df1['BALANCE'].hist(bins=10, color='yellow', ax=axes[1])
# Create a blue bar histogram of 'BALANCE' column with 10 bins # Data source is the 'BALANCE' column of DataFrame df1

# Display the plots
plt.show()

"""Similar to above we apply on all columns"""

fig, axes = plt.subplots(1,2,figsize=(12,5),dpi = 90)
# count of col (countplot)
sns.boxplot(x =df1.BALANCE_FREQUENCY, data = df1,showmeans=True, orient='h',color="orange",ax=axes[0] )
df1['BALANCE_FREQUENCY'].hist(bins = 10,color='yellow',ax=axes[1])
plt.show()

# Filtering the DataFrame df1 to include only rows where BALANCE_FREQUENCY is less than 0.1
df1__ = df1['BALANCE_FREQUENCY'] < 0.1
# Counting the number of True values after filtering
sum(df1__)

# Finding the 10 smallest values in the BALANCE_FREQUENCY column
df1['BALANCE_FREQUENCY'].nsmallest(10)

# Generating descriptive statistics for the BALANCE_FREQUENCY column
df1['BALANCE_FREQUENCY'].describe()

"""
The values in the 'BALANCE_FREQUENCY' column equal to zero are not considered noise or outliers, as they are not significantly distant from the rest of the data points."""

fig, axes = plt.subplots(1,2,figsize=(12,5),dpi = 90)

# count of col (countplot)

sns.boxplot(x =df1.PURCHASES, showmeans=True, orient='h',color="orange",data = df1,ax=axes[0] )

df1['PURCHASES'].hist(bins = 10,color='yellow',ax=axes[1])

plt.show()

#PURCHASES > 30000
df1[df1["PURCHASES"]>30000]

df1['PURCHASES'].describe()

# Filter rows where purchase amount exceeds 30000
df1__ = df1['PURCHASES'] > 30000

# Describe "PURCHASES" column for rows where purchase amount is not greater than 30000
df1[df1__ == False]['PURCHASES'].describe()

"""There is a noteworthy observation regarding one particular sample (index = 512), where both the PURCHASES and PAYMENTS fall within the outlier range. Additionally, potential outliers noticeably influence the statistics, particularly the mean value of this column. These potential outliers are located at indices 246, 464, 512, 1166, 1509, 1545, and 3792."""

fig, axes = plt.subplots(1,2,figsize=(12,5),dpi = 90)

# count of col (countplot)

sns.boxplot(x =df1.ONEOFF_PURCHASES, showmeans=True, orient='h',color="orange",data = df1,ax=axes[0] )

df1['ONEOFF_PURCHASES'].hist(bins = 10,color='yellow',ax=axes[1])

plt.show()

#ONEOFF_PURCHASES > 30000
df1['ONEOFF_PURCHASES'].nlargest(8)

df1[df1["ONEOFF_PURCHASES"]>30000]

"""Upon examining the samples identified by this filter alongside those from the previous feature, it's evident that all the outliers detected in the ONEOFF_PURCHASES column are also outliers in the PURCHASES column. These outliers are located at indices 464, 512, 1509, and 3792."""

fig, axes = plt.subplots(1,2,figsize=(12,5),dpi = 90)

# count of col (countplot)

sns.boxplot(x =df1.INSTALLMENTS_PURCHASES, showmeans=True, orient='h',color="orange",data = df1,ax=axes[0] )

df1['INSTALLMENTS_PURCHASES'].hist(bins = 10,color='yellow',ax=axes[1])

plt.show()

#INSTALLMENTS_PURCHASES > 20000
df1['INSTALLMENTS_PURCHASES'].nlargest(8)

df1['INSTALLMENTS_PURCHASES'].describe()

mask = df1['INSTALLMENTS_PURCHASES'] < 20000
df1[mask]['INSTALLMENTS_PURCHASES'].describe()

df1.sort_values(by = 'INSTALLMENTS_PURCHASES', ascending = False).head(1)

"""The outlier at index 5086 stands out due to its considerable distance from the nearest neighboring samples, resulting in a notable change in column statistics upon its removal."""

fig, axes = plt.subplots(1,2,figsize=(12,5),dpi = 90)

# count of col (countplot)

sns.boxplot(x =df1.CASH_ADVANCE, showmeans=True, orient='h',color="orange",data = df1,ax=axes[0] )

df1['CASH_ADVANCE'].hist(bins = 10,color='yellow',ax=axes[1])

plt.show()

#CASH_ADVANCE > 40000
df1[df1['CASH_ADVANCE']>40000]

df1.loc[df1['CASH_ADVANCE'].nlargest(5).index]

df1['CASH_ADVANCE'].describe()

mask = df1['CASH_ADVANCE'] < 45000
df1[mask]['CASH_ADVANCE'].describe()

df1.loc[df1['CASH_ADVANCE'].nlargest(5).index]

"""
The sole outlier in this column, located at index 2054, stands out due to its considerable distance from other samples, thus exerting a significant influence on the column's statistics."""

fig, axes = plt.subplots(1,2,figsize=(12,5),dpi = 90)

# count of col (countplot)

sns.boxplot(x =df1.PURCHASES_FREQUENCY, showmeans=True, orient='h',color="pink",data = df1,ax=axes[0] )

sns.histplot(df1.PURCHASES_FREQUENCY,kde=True,color='darkred',ax=axes[1])

plt.show()

"""that is normal.(This feature is discrete)"""

fig, axes = plt.subplots(1,3,figsize=(12,5),dpi = 90)

# count of col (countplot)

sns.boxplot(x =df1.CASH_ADVANCE_FREQUENCY, showmeans=True, orient='h',color="pink",data = df1,ax=axes[0] )

df1['CASH_ADVANCE_FREQUENCY'].hist(bins = 10,color='green',ax=axes[1])
sns.kdeplot(df1.CASH_ADVANCE_FREQUENCY, shade=True,ax=axes[2] ,color ='blue')
plt.show()

"""ASH_ADVANCE_FREQUENCY > 1.4"""

df1[df1['CASH_ADVANCE_FREQUENCY']>1.4]

df1['CASH_ADVANCE_FREQUENCY'].nlargest(5)

df1['CASH_ADVANCE_FREQUENCY'].describe()

mask1 = df1['CASH_ADVANCE_FREQUENCY']<1.4
df1[mask]['CASH_ADVANCE_FREQUENCY'].describe()

"""
Since the potential outlier in this column isn't significantly distant from the rest of the values and considering the small range of values, it doesn't have a major impact on the statistics. Moreover, in real-world scenarios, this value is plausible."""

fig, axes = plt.subplots(1,3,figsize=(12,5),dpi = 90)

# count of col (countplot)

sns.boxplot(x =df1.CASH_ADVANCE_TRX, showmeans=True, orient='h',color="pink",data = df1,ax=axes[0] )

df1['CASH_ADVANCE_TRX'].hist(bins = 10,color='green',ax=axes[1])
sns.kdeplot(df1.CASH_ADVANCE_TRX, shade=True,ax=axes[2] ,color ='blue')
plt.show()

"""CASH_ADVANCE_TRX > 80"""

df1[df1['CASH_ADVANCE_TRX']>80]

df1['CASH_ADVANCE_TRX'].describe()

mas = df1['CASH_ADVANCE_TRX'] <= 80
df1[mas]['CASH_ADVANCE_TRX'].describe()

"""
The outliers in this column had a minor impact on the mean value but significantly influenced the standard deviation. These outliers are located at indices 504, 2054, 8039, 3417, 1812, and 5113."""

fig, axes = plt.subplots(1,3,figsize=(12,5),dpi = 90)

# count of col (countplot)

sns.boxplot(x =df1.CREDIT_LIMIT, showmeans=True, orient='h',color="pink",data = df1,ax=axes[0] )

df1['CREDIT_LIMIT'].hist(bins = 10,color='green',ax=axes[1])
sns.kdeplot(df1.CREDIT_LIMIT, shade=True,ax=axes[2] ,color ='blue')
plt.show()

"""CREDIT_LIMIT >= 25000"""

df1[df1['CREDIT_LIMIT']>= 25000]

df1['CREDIT_LIMIT'].describe()

mask_ = df1['CREDIT_LIMIT'] <= 25000
df1[mask_]['CREDIT_LIMIT'].describe()

"""
While the boxplot indicates the presence of some outliers, it's worth noting that the potential outliers are not significantly distant from the rest of the samples."""

fig, axes = plt.subplots(1,3,figsize=(12,5),dpi = 90)

# count of col (countplot)

sns.boxplot(x =df1.PAYMENTS, showmeans=True, orient='h',color="pink",data = df1,ax=axes[0] )

df1['PAYMENTS'].hist(bins = 10,color='green',ax=axes[1])
sns.kdeplot(df1.PAYMENTS, shade=True,ax=axes[2] ,color ='blue')
plt.show()

"""PAYMENTS > 45000"""

df1[df1['PAYMENTS']>= 45000]

df1['PAYMENTS'].describe()

mask = df1['PAYMENTS'] < 45000
df1[mask]['PAYMENTS'].describe()

"""
Indices 512 and 4072."""

fig, axes = plt.subplots(1,2,figsize=(12,5),dpi = 90)

# count of col (countplot)

sns.boxplot(x =df1.MINIMUM_PAYMENTS, showmeans=True, orient='h',color="orange",data = df1,ax=axes[0] )

df1['MINIMUM_PAYMENTS'].hist(bins = 10,color='yellow',ax=axes[1])
plt.show()

"""MINIMUM_PAYMENTS > 35000"""

df1[df1['MINIMUM_PAYMENTS']> 35000]

df1['MINIMUM_PAYMENTS'].describe()

mask = df1['MINIMUM_PAYMENTS'] > 35000
df1[mask == False]['MINIMUM_PAYMENTS'].describe()

"""index = 4226, 6902, 5474, 4311"""

# Create a copy of DataFrame df1 named df2
df2 = df1.copy()

# Remove rows with specified indices from df2
df2 = df1.drop([246, 464, 512, 1166, 1509, 1545, 3792, 3792, 5086, 2054, 504, 8039, 3417, 1812, 5113,
                4072, 4226, 6902, 5474, 4311])

# Reset the index of df2
df2.reset_index(drop=True, inplace=True)

# Create a copy of DataFrame df2 named df3
df3 = df2.copy()

# Display df2
df2

"""Analyzing the correlation between features reveals several pairs with high correlation, which may pose challenges for modeling. These pairs include:

ONEOFF_PURCHASES and PURCHASES PURCHASES_INSTALLMENTS_FREQUENCY and PURCHASES_FREQUENCY
CASH_ADVANCE_TRX and CASH_ADVANCE_FREQUENCY
High correlation between features can lead to collinearity issues, making it difficult to accurately assess their individual impact on the dependent variable. This can result in beta coefficients that appear unreasonable. Features with high correlation are more linearly dependent, thus having a similar effect on the dependent variable. Therefore, when two features have high correlation, it's advisable to drop one of them to mitigate collinearity issues.






"""

# Create a heatmap to visualize the correlation matrix of df1
fig, ax = plt.subplots(figsize=(10, 8), dpi=80)
sns.heatmap(df1.corr(), center=1, cbar=True, annot=True, linewidths=0.5, ax=ax)
plt.show()  # Display the heatmap

"""**Result of cheaking the noise**

The potential noisy samples (with indices 246, 464, 512, 1166, 1509, 1545, 3792, 3792, 5086, 2054, 504, 8039, 3417, 1812, 5113, 4072, 4226, 6902, 5474, and 4311) will be excluded from the main dataset and kept in a separate dataframe. This approach allows for their exclusion from certain algorithms while still enabling their inclusion in others, especially density-based clustering methods that are robust to outliers.

**More calculative measures to get more information about customer**

*Monthly Average Purchase*
**Monthly_Avg_Purchase=Purchase / Tenure**
"""

# Calculate the average monthly purchase for each customer
Monthly_Avg_Purchase = df3['PURCHASES'] / df3['TENURE']

# Add the Monthly Average Purchase column to the DataFrame df3
df3['Monthly_Avg_Purchase'] = df3['PURCHASES'] / df3['TENURE']

# Display the updated DataFrame df3
df3

"""*Monthly Average Cash Advance Amount*
**Monthly Average Cash = CASH_ADVANCE/TENURE**

"""

#The average monthly cash advance for the customers
Monthly_Avg_Cash = df3['CASH_ADVANCE']/df3['TENURE']
df3['Monthly_Avg_Cash'] = df3['CASH_ADVANCE']/df3['TENURE']

"""*Division of Customers based on the type of Purchases (One-Off, Installments)*

To comprehend customer spending patterns based on the type of purchases, particularly distinguishing between one-off purchases and purchases made in installments. This categorization aims to identify whether customers predominantly make one-off purchases, primarily utilize installment plans, engage in both types of purchases, or exhibit no clear spending behavior in either category.
"""

# Step 1: Separating the Type of Purchases data into another dataframe
df_purchases = df3[['ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES']]

# Displaying the dataframe containing the Type of Purchases data
df_purchases

df_purchases.info()

# Step 2: Filtering the categories and counting the occurrences:

# 1. Counting customers who made neither one-off purchases nor purchases in installments
df_purchases[(df_purchases['ONEOFF_PURCHASES'] == 0) & (df_purchases['INSTALLMENTS_PURCHASES'] == 0)].shape

# 2. Counting customers who made only one-off purchases
df_purchases[(df_purchases['ONEOFF_PURCHASES'] > 0) & (df_purchases['INSTALLMENTS_PURCHASES'] == 0)].shape

# 3. Counting customers who made only purchases in installments
df_purchases[(df_purchases['ONEOFF_PURCHASES'] == 0) & (df_purchases['INSTALLMENTS_PURCHASES'] > 0)].shape

# 4. Counting customers who made both one-off purchases and purchases in installments
df_purchases[(df_purchases['ONEOFF_PURCHASES'] > 0) & (df_purchases['INSTALLMENTS_PURCHASES'] > 0)].shape

"""We observe that the total **8949** customers with credit cards are divided into **4 categories** based on their purchase types: one-off purchases, purchases in installments, both types of purchases, or no purchases at all.

Therefore, we will create categories to group customers based on their purchase types (One-Off, Installments). The 4 categories are:

1. Customers making purchases in both categories *(Both_the_Purchases)*
2. Customers making purchases only in installments *(Installment_Purchases)*
3. Customers making purchases only as one-off transactions *(One_Of_Purchase)*
4. Customers making no purchases at all *(None_Of_the_Purchases)*
"""

# Define Purchase_Type based on purchase categories
df3['Purchase_Type'] = np.where((df3['ONEOFF_PURCHASES'] == 0) & (df3['INSTALLMENTS_PURCHASES'] == 0),
                                'None_Of_the_Purchases',
                                np.where((df3['ONEOFF_PURCHASES'] > 0) & (df3['INSTALLMENTS_PURCHASES'] == 0),
                                         'One_Of_Purchase',
                                         np.where((df_purchases['ONEOFF_PURCHASES'] == 0) & (df_purchases['INSTALLMENTS_PURCHASES'] > 0),
                                                  'Installment_Purchases',
                                                  'Both_the_Purchases')))

# Display the counts of each Purchase_Type category
df3['Purchase_Type'].value_counts()

# Plotting the distribution of customers based on Purchase Type
df3['Purchase_Type'].value_counts().sort_index().plot(kind='pie', autopct='%1.01f%%',
                                                      colors=['#87d42f', '#d33243', '#292f67', '#FFD700'],
                                                      fontsize=10, textprops={'fontsize': 18})
plt.title('Distribution of Customers based on the Purchase Type')
plt.show()

"""* 31.7% of customers engage in both One-Off and Installment Purchases, while 24.9% exclusively make installment purchases.
* The dataset includes attributes PURCHASES_TRX for average amount per purchase transaction and CASH_ADVANCE_TRX for average amount per cash advance transaction.

*Estimating the Limit Usage of customers*
A higher utilization rate indicates the presence of credit risk, while a lower utilization rate is desirable.

**Balance-to-Limit Ratio= Credit Limit/Balance**

â€‹This ratio helps assess how much of the available credit a customer is using. A lower ratio suggests that the customer is using less of their available credit, which is typically viewed positively in terms of credit risk.
"""

df3['Limit_Usage'] = df3['BALANCE']/df3['CREDIT_LIMIT']

"""*Payments to Minimum_Payments Ratio*
The Payments to Minimum Payments Ratio compares the amount paid towards the credit card balance to the minimum required payment.

**Pay_to_MinimumPay = PAYMENTS/MINIMUM_PAYMENTS**
"""

df3['Pay_to_MinimumPay'] = df3['PAYMENTS']/df3['MINIMUM_PAYMENTS']
df3['Pay_to_MinimumPay']

"""Exploring the data using the Purchase_Type feature alongside other attributes will provide insights into customer profiles and behavior. This analysis will help us understand how customers' purchasing patterns differ based on the types of purchases they make.

**Exploring Customer Behavior Based on Purchase Types and Attributes**

*Pay_to_MinimumPay over Purchase Type*

Ratio of payments made to minimum payments due, categorized by Purchase Type.
"""

# Step 1: Calculate the average of Pay_to_MinimumPay for each Purchase Type
average_pay_to_min_ratio = df3.groupby(by=['Purchase_Type'])['Pay_to_MinimumPay'].mean().sort_values(ascending=False)

t1 = average_pay_to_min_ratio
# Display the results
t1

# Step 2: Plot the graph
plt.figure(figsize=(10, 6))
t1.plot(kind='bar', color='darkblue')
plt.title('Distribution of Average Payments to Minimum Payments ratio for Purchase Type categories', fontsize=15)
plt.xlabel('Purchase Types', fontsize=10)
plt.ylabel('Payments to Minimum Payments ratio', fontsize=10)
plt.xticks(rotation=0,fontsize=10)
plt.show()

"""Among the Purchase Type categories, customers who made installment purchases paid the highest average minimum payment dues.

*Balance to Credit Limit ratio (or Utilization rate) over Purchase Type*

*Balance-to-Credit-Limit Ratio (Utilization Rate) by Purchase Type*
"""

# Step 1: Calculate the average Limit Usage (credit card score) for each Purchase Type
average_limit_usage = df3.groupby(['Purchase_Type'])['Limit_Usage'].mean().sort_values(ascending=True).reset_index()

t2 = average_limit_usage

# Display the results
t2

# Step 2: Plot the graph of Average Utilization rate over Purchase Type
plt.figure(figsize=(10, 6))  # Increase the figure size
sns.barplot(x=t2['Purchase_Type'], y=t2['Limit_Usage'], palette='tab10')
plt.title('Average Utilization rate over Purchase Type', fontsize=15)
plt.xlabel('Purchase Types', fontsize=12)
plt.ylabel('Balance to Credit Limit ratio', fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.show()

"""Customers who make installment purchases exhibit the lowest balance-to-limit ratio, which is desirable as it indicates lower credit risk.

*Monthly_Avg_Purchase over Purchase Type*

Average monthly purchase amount categorized by Purchase Type.
"""

df4=df3.copy()
df4.head()

# Step 1: Calculate the average of Monthly Average Purchase for each Purchase Type
average_monthly_purchase = df3.groupby(by=['Purchase_Type'])['Monthly_Avg_Purchase'].mean().sort_values(ascending=False)

t3 = average_monthly_purchase

# Display the results
t3

# Step 2: Plot the graph
plt.figure(figsize=(8, 6))
t3.plot(kind='bar', color='yellow')
plt.title('Average Monthly Average Purchase over Purchase Type')
plt.xlabel('Purchase Types')
plt.ylabel('Monthly Average Purchase')
plt.xticks(rotation=0,fontsize=8)
plt.show()

"""The highest total average purchase amount over the last 12 months is made by customers who have made both one-off and installment purchases.

*Monthly_Avg_Purchase over Purchase Type*

Average monthly purchase amount categorized by Purchase Type.
"""

# Step 1: Calculate the average of Monthly Average Cash Advance for each Purchase Type
average_monthly_cash_advance = df3.groupby(['Purchase_Type'])['Monthly_Avg_Cash'].mean().sort_values(ascending=False).reset_index()

t4 = average_monthly_cash_advance

# Display the results
t4

# Plot the graph
sns.barplot(x=t4['Purchase_Type'], y=t4['Monthly_Avg_Cash'], palette='tab10')
plt.title('Average Monthly Average Cash Advance Amount by Purchase Type')
plt.xlabel('Purchase Types')
plt.ylabel('Monthly Cash Advance Amount')
plt.xticks(rotation=0,fontsize=8)
plt.show()

"""
Customers who made neither one-off nor installment purchases have the highest monthly average cash in advance amount."""

# Calculate the correlation matrix
corr_df = df3.corr()

# Display the correlation matrix
corr_df

"""Dropping the original variables 'BALANCE', 'PURCHASES', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'TENURE', and 'CASH_ADVANCE', as they were used to create new variables. These original variables may have high correlation with the derived variables, causing redundancy in the dataset."""

df5 = df3.copy()
# Dropping the original variables 'BALANCE','PURCHASES','PAYMENTS','MINIMUM_PAYMENTS','TENURE','CASH_ADVANCE'
df5.drop(['BALANCE','CREDIT_LIMIT','PURCHASES','PAYMENTS','MINIMUM_PAYMENTS','TENURE','CASH_ADVANCE'], axis=1, inplace=True)

### Finding Correlation among the variables:
plt.figure(figsize=(14, 8))
sns.heatmap(round(df5.corr(), 2), annot=True, cmap='coolwarm', linewidths=3, fmt='.2g')
plt.title('Correlation Matrix')
plt.show()

"""*The correlation value above 0.50 is considered highly correlated. Observing the heatmap:*

- OneOFF_Purchases is highly positively correlated with Monthly_Avg_Purchase at a correlation of 0.91.
- Installment_Purchases is correlated with Purchases_Trx at a correlation of 0.63, and with Monthly_Avg_Purchase at a correlation of 0.68.
- Purchases_Frequency is positively correlated with Purchases_Installment_Frequency at a correlation of 0.86, and also with Purchases_Trx at a correlation of 0.57.
- Cash_Advance_Frequency is highly positively correlated with Cash_Advance_Trx at a correlation of 0.80, and with Monthly_Avg_Cash at a correlation of 0.63.
- Cash_Advance_Trx is also positively correlated with Monthly_Avg_Cash at a correlation of 0.63.
- Purchases_Trx is positively correlated with Monthly_Avg_Purchase at a correlation of 0.68.

**Creatinag a Model**
"""

# Create dummy variables for Purchase_Type using pd.get_dummies()
x_cat = pd.get_dummies(df5['Purchase_Type'], drop_first=True)  # Drop the first category to avoid multicollinearity
x_cat  # Display the resulting dummy variables dataframe

# Filter numerical variables
x_num = df5.dtypes[df5.dtypes != 'object'].index.to_list()  # Select columns with non-object data type
x_num  # Display numerical variable names

# Filter numerical variables in the original df from df4 (Copy of df):
x_num_df4 = df4.dtypes[df4.dtypes != 'object'].index.to_list()
x_num_df4

# Concatenate original variables with dummy variables without scaling the numerical variables:
original_df = pd.concat([x_cat, df4[x_num_df4]], axis=1)
original_df.head()

"""**Data Normalization**  : Normalization scales numeric features to a common range, ensuring consistency in distances between data points, which is crucial for clustering algorithms."""

# Initialize the StandardScaler
SS = StandardScaler()

# Fit and transform the numerical variables using StandardScaler
# StandardScaler standardizes features by removing the mean and scaling to unit variance
x_scaled = pd.DataFrame(SS.fit_transform(df5[x_num]), columns=x_num)
x_scaled.head()

# Combine the categorical and scaled numerical datasets
concat_df = pd.concat([x_cat, x_scaled], axis=1)
concat_df.head()

"""**P C A**

* PCA (Principal Component Analysis) is a dimensionality reduction technique.
* It identifies new axes or dimensions for a dataset.
* These axes are chosen to explain the maximum variance in the data.
* The first principal component explains the most variance.
* Subsequent components are orthogonal to preceding ones and explain the maximum remaining variance.
* PCA allows for the reduction of the original feature space to a lower-dimensional space while retaining as much variance as possible.





"""

# Initialize PCA model with 17 components
pca_model = PCA(n_components=17)

# Fit PCA model and transform the concatenated dataframe
X_PCA = pca_model.fit_transform(concat_df)

# Calculate cumulative variance explained by the principal components
pca_var = pca_model.explained_variance_ratio_
np.cumsum(pca_var)

# Calculate cumulative variance explained by the principal components
var1 = np.cumsum(np.round(pca_model.explained_variance_ratio_, decimals=6) * 100)
var1

# Create a summary table showing Eigen Vectors, Eigen Values, and variance explained by each component
vec_val = pd.DataFrame({'Eigen_Values': pca_model.explained_variance_,
                        'Cumulative_Variance': var1},
                       index=range(1, 18)).round(4)
vec_val

"""Conclusion:
* We chose 7 components, which collectively explain 85% of the variation. Each individual component explains more than 0.7 variance.
* By selecting these 7 dimensions out of the 17 variables, we lose only about 15% of the variation (information) of the data.
"""

# Perform PCA with 8 components
PCA_7 = PCA(n_components=7)
X_PCA_7 = PCA_7.fit_transform(concat_df)

# Create a dataframe to store the principal components
PC = pd.DataFrame(X_PCA_7, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7'])
PC

"""Following are the Principal components taking all the 17 variables:"""

# Extracting the list of columns from the concatenated dataframe
list_cols = concat_df.columns

# Create a dataframe to store the principal components along with all variables
PC_with_all_variables = pd.DataFrame(PCA_7.components_.T,
                                    columns=['PC_' + str(i) for i in range(1, 8)],
                                    index=list_cols)
PC_with_all_variables

# Exporting the dataframe containing principal components with all variables to a CSV file
PC_with_all_variables.to_csv('PC_with_all_variables.csv')

# Variance explained by each of the components
pd.Series(PCA_7.explained_variance_ratio_ * 100, index=['PC_' + str(i) for i in range(1, 8)])

"""**Determining the Optimal Number of Clusters using KMeans**

KMeans is a popular clustering algorithm used to partition data into distinct groups based on similarities in features. It works by iteratively assigning data points to the nearest cluster centroid and updating centroids to minimize the within-cluster sum of squared distances.

* Fit KMeans with different cluster numbers.
*Calculate inertia for each cluster number.
* Plot cluster number against inertia and identify the elbow point.
* Alternatively, compute silhouette score for each cluster number.
* Choose the number of clusters at the elbow point or with maximum silhouette score.
* This ensures effective clustering while avoiding overfitting.
"""

# Finding the Optimal clusters using KMeans, Silhouette Coefficient Score for KMeans

wcss = []  # List to store within-cluster sum of squares
sil_kmeans = []  # List to store silhouette scores

# Iterate over different numbers of clusters
for i in range(3, 9):

    # K-Means Clustering
    kmeans = KMeans(n_clusters=i, n_init=100, init='k-means++', random_state=0)
    kmeans.fit(X_PCA_7)

    # Calculate Inertia and Silhouette Score for Clusters using K-Means:
    in_km = kmeans.inertia_
    wcss.append(in_km)
    sil_km = silhouette_score(X_PCA_7, kmeans.labels_)
    sil_kmeans.append(sil_km)

    # Print results
    print('Number of clusters:', i)
    print('KMeans Inertia:', in_km)
    print('Silhouette Score for KMeans:', sil_km)
    print()

# Importing necessary libraries and modules
from yellowbrick.cluster import SilhouetteVisualizer
import matplotlib.pyplot as plt

# Setting the style for plotting
plt.style.use('seaborn-paper')

# Creating subplots grid with 2 rows and 3 columns
fig, axs = plt.subplots(2, 3, figsize=(20, 15))
axs = axs.reshape(6)

# Iterating over different numbers of clusters
for i, k in enumerate(range(3, 9)):
    # Retrieving the corresponding axis for visualization
    ax = axs[i]

    # Creating SilhouetteVisualizer instance for KMeans clustering
    sil = SilhouetteVisualizer(KMeans(n_clusters=k, n_init=100, init='k-means++', random_state=0), ax=ax)

    # Fitting the visualizer to the PCA-transformed data and finalizing the visualization
    sil.fit(X_PCA_7)
    sil.finalize()

"""***Elbow Method***-

**WCSS Calculation:** The elbow method involves calculating the within-cluster sum of squares (WCSS) for various numbers of clusters.
- **Cluster Tightness:** WCSS measures how tightly data points are clustered around their centroids.
- **Elbow Curve:** Plotting WCSS against the number of clusters generates an elbow curve.
- **Curve Behavior:** The curve typically decreases as the number of clusters increases.
- **Elbow Point:** The optimal number of clusters is identified at the point where the curve starts to flatten out.
- **Trade-off:** Choosing the number of clusters involves balancing capturing important patterns in the data and avoiding overfitting.
- **Decision Point:** The elbow method aids in selecting an appropriate number of clusters by identifying the point where the WCSS improvement levels off.
- **Additional Analysis:** Sometimes, determining the optimal number of clusters may require supplementary techniques if the elbow point is not clearly discernible.
"""

# Plotting graph of Elbow Method
plt.figure(figsize=(10,6))
plt.plot(range(3,9), wcss, c ='#336699', marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Clusters Errors')
plt.show()

#The elbow point, where the curve begins to flatten out, suggests the optimal number of clusters.

"""The "inertia" refers to the sum of squared distances of each data point within a cluster from its respective centroid. Utilizing the elbow method, the optimal number of clusters (K) is determined by identifying the point at which the inertia stops dropping significantly."""

# Plotting the Silhouette Scores for K-Means Clustering
plt.figure(figsize=(8,6))
plt.plot(range(3,9), sil_kmeans, marker='s', c='yellow')
plt.title('Silhouette Scores for K-Means Clustering')  # Set title
plt.xlabel('Number of Clusters')  # Label x-axis
plt.ylabel('Silhouette Score')  # Label y-axis
plt.grid()  # Add grid lines
plt.show()  # Display the plot

"""**K-Means Clusters: For K= 3**

This code applies K-Means clustering with K=3, then visualizes the spread of the data using scatter plots for each pair of principal components. The clusters are represented by different colors.






"""

# Applying K-Means Clustering with K=3
kmeans = KMeans(n_clusters=3, n_init=100, init='k-means++', random_state=0)
kmeans.fit(X_PCA_7)

# Creating DataFrame for PCA components and cluster labels
df_pca = pd.DataFrame(X_PCA_7)
y_lab = pd.Series(kmeans.labels_, name='y')

# Concatenating the dataframes
df_final = pd.concat([df_pca, y_lab], axis=1)

# Visualizing the spread of the data for each pair of principal components
for i in range(6):
    print('Scatter plot for Principal Components', i, 'and', i+1)
    sns.scatterplot(x=df_pca[i], y=df_pca[i+1], hue=df_final['y'], palette=['blue', 'red', 'green'])
    plt.show()

# Creating K-Means clusters with K=3
km_3 = KMeans(n_clusters=3, n_init=100, init='k-means++', random_state=0)
KM_3 = km_3.fit(X_PCA_7)

# Accessing the cluster labels assigned by KMeans clustering
KM_3.labels_

# Retrieving the centroids for Cluster 3 from the KMeans model
KM_3.cluster_centers_

# Appending the cluster labels obtained from KMeans clustering to the original dataframe
original_df['cluster_3'] = KM_3.labels_

# Appending the cluster labels obtained from KMeans clustering to the original dataframe
original_df['cluster_3'] = KM_3.labels_

# the new data set has Original variables + the Cluster Labels from each of the clusters got from K-Means
original_df.head()

"""Similar to above
**K-Means Clusters: For K= 4**
"""

# Applying Clustering and visualizing the spread of the data (finding out if the data points have been clustered correctly through visualization)

# K-Means Clusters: For K= 4
kmeans = KMeans(n_clusters=4, n_init=100, init='k-means++', random_state=0)
kmeans.fit(X_PCA_7)

# Taking into each dataframes
df_pca = pd.DataFrame(X_PCA_7)
y_lab = pd.Series(kmeans.labels_, name='y')  # labels for clusters

# concatenating the dataframe:
df_final = pd.concat([df_pca, y_lab], axis=1)

# As there are 7 dimensions, hence we need to plot for each of the different pairs to visualize the spread of the data:
for i in range(6):
    print('Scatter plot for Principal Components', i, 'and', i+1)
    sns.scatterplot(x=df_pca[i], y=df_pca[i+1], hue=df_final['y'], palette=['yellow', 'blue', 'red', 'green'])
    plt.show()

km_4 = KMeans(n_clusters=4, n_init=100, init='k-means++', random_state=0)
KM_4 = km_4.fit(X_PCA_7)

KM_4.labels_

# Centroids for Cluster 4:
KM_4.cluster_centers_

"""**K-Means Clusters: For Clsuters for K = 5, 6,7,8**"""

# Fit KMeans models with different numbers of clusters (4 to 8)
KM_4 = KMeans(n_clusters=4, n_init=100, init='k-means++', random_state=0).fit(X_PCA_7)  # Fit KMeans model with 4 clusters
KM_5 = KMeans(n_clusters=5, n_init=100, init='k-means++', random_state=0).fit(X_PCA_7)  # Fit KMeans model with 5 clusters
KM_6 = KMeans(n_clusters=6, n_init=100, init='k-means++', random_state=0).fit(X_PCA_7)  # Fit KMeans model with 6 clusters
KM_7 = KMeans(n_clusters=7, n_init=100, init='k-means++', random_state=0).fit(X_PCA_7)  # Fit KMeans model with 7 clusters
KM_8 = KMeans(n_clusters=8, n_init=100, init='k-means++', random_state=0).fit(X_PCA_7)  # Fit KMeans model with 8 clusters

# Append the cluster labels to the original data (not standardized data)
original_df['cluster_3'] = KM_3.labels_
original_df['cluster_4'] = KM_4.labels_
original_df['cluster_5'] = KM_5.labels_
original_df['cluster_6'] = KM_6.labels_
original_df['cluster_7'] = KM_7.labels_
original_df['cluster_8'] = KM_8.labels_
# The new dataset contains original variables along with cluster labels

# Displaying the new dataset containing original variables along with cluster labels
original_df.head()

# Calculating the distribution of segments for cluster K = 3
segment_distribution_k3 = pd.Series.sort_index(original_df.cluster_3.value_counts()) / sum(original_df.cluster_5.value_counts())
segment_distribution_k3

# Calculating the distribution of segments for cluster K = 4
segment_distribution_k4 = pd.Series.sort_index(original_df.cluster_4.value_counts()) / sum(original_df.cluster_5.value_counts())
segment_distribution_k4

# Calculating the distribution of segments for cluster K = 5
segment_distribution_k5 = pd.Series.sort_index(original_df.cluster_5.value_counts()) / sum(original_df.cluster_5.value_counts())
segment_distribution_k5

# Calculating the distribution of segments for cluster K = 6
segment_distribution_k6 = pd.Series.sort_index(original_df.cluster_6.value_counts()) / sum(original_df.cluster_6.value_counts())
segment_distribution_k6

# Calculating the distribution of segments for cluster K = 7
segment_distribution_k7 = pd.Series.sort_index(original_df.cluster_7.value_counts()) / sum(original_df.cluster_7.value_counts())
segment_distribution_k7

# Calculating the distribution of segments for cluster K = 8
segment_distribution_k8 = pd.Series.sort_index(original_df.cluster_8.value_counts()) / sum(original_df.cluster_8.value_counts())
segment_distribution_k8

# Getting the total number of observations in cluster 3
original_df.cluster_3.size

# Step 1b: Get the breakdown of the values in each segment:
# This shows how many observations are there in each respective segment:

original_df.cluster_3.value_counts()

# Sorting the index of the value counts provides:
# The value counts based on the Segment Label (0, 1, 2 depending upon the K-value) in the index
# and not based on the highest value within the segments

pd.Series.sort_index(original_df.cluster_3.value_counts())

# Combining the size for each cluster K value into one single array:

size = pd.concat([pd.Series(original_df.cluster_3.size),
                  pd.Series.sort_index(original_df.cluster_3.value_counts()),
                  pd.Series.sort_index(original_df.cluster_4.value_counts()),
                  pd.Series.sort_index(original_df.cluster_5.value_counts()),
                  pd.Series.sort_index(original_df.cluster_6.value_counts()),
                  pd.Series.sort_index(original_df.cluster_7.value_counts()),
                  pd.Series.sort_index(original_df.cluster_8.value_counts())])

size
# Gives the size of Segments for each of the Clusters :

# Segment Size:
Seg_size=pd.DataFrame(size, columns=['Seg_size'])

# Segment Distribtuion % wise:
Seg_Pct = pd.DataFrame(size/original_df.cluster_3.size, columns=['Seg_Pct'])

# Taking Transpose of Segment Percentage :
Seg_Pct.T

# Concatenating the Segment Size and Segment Percentage:
pd.concat([Seg_size.T, Seg_Pct.T], axis=0)

"""* Compute the overall average for each variable across the entire dataset.
* Calculate the average for each variable within each cluster.

A good indication of the distribution of data is the mean value, hence will find the average value for each variable and for each cluster.  
"""

# Overall each variables wise Avg:
original_df.apply(np.mean).T

# Grouping-by over each cluster to find the Segment wise average for each variable
original_df.groupby('cluster_3').apply(np.mean).T

# Concatenating the overall average values for each variable with the segment-wise average values for each variable across different cluster sizes obtained from KMeans clustering

Profiling_output = pd.concat([
    # Overall average values for each variable
    original_df.apply(lambda x: x.mean()).T,
    # Segment-wise average values for each variable across cluster_3
    original_df.groupby('cluster_3').apply(lambda x: x.mean()).T,
    # Segment-wise average values for each variable across cluster_4
    original_df.groupby('cluster_4').apply(lambda x: x.mean()).T,
    # Segment-wise average values for each variable across cluster_5
    original_df.groupby('cluster_5').apply(lambda x: x.mean()).T,
    # Segment-wise average values for each variable across cluster_6
    original_df.groupby('cluster_6').apply(lambda x: x.mean()).T,
    # Segment-wise average values for each variable across cluster_7
    original_df.groupby('cluster_7').apply(lambda x: x.mean()).T,
    # Segment-wise average values for each variable across cluster_8
    original_df.groupby('cluster_8').apply(lambda x: x.mean()).T
], axis=1)

# Display the resulting DataFrame
Profiling_output

# Combining the outputs from steps 1 and 2:
# Concatenating the segment size, segment distribution, the overall averages, and the individual segment-wise average

Profiling_output_final = pd.concat([Seg_size.T, Seg_Pct.T, Profiling_output], axis=0)

# Adding column names to the final profiling output
Profiling_output_final.columns = ['Overall', 'KM3_1', 'KM3_2', 'KM3_3',
                                  'KM4_1', 'KM4_2', 'KM4_3', 'KM4_4',
                                  'KM5_1', 'KM5_2', 'KM5_3', 'KM5_4', 'KM5_5',
                                  'KM6_1', 'KM6_2', 'KM6_3', 'KM6_4', 'KM6_5', 'KM6_6',
                                  'KM7_1', 'KM7_2', 'KM7_3', 'KM7_4', 'KM7_5', 'KM7_6', 'KM7_7',
                                  'KM8_1', 'KM8_2', 'KM8_3', 'KM8_4', 'KM8_5', 'KM8_6', 'KM8_7', 'KM8_8']

# Display the final profiling output
Profiling_output_final

"""Based on the clustering analysis, we have identified 5 distinct clusters that provide the best solution for segmenting customers. Here's a summary of each segment:

1. **Big Tickets**: Customers in this segment make purchases very frequently and in large amounts. They are likely to be high-value customers who contribute significantly to revenue.
   
2. **Medium Tickets**: These customers prefer to make installment purchases and do so frequently. While they may not spend as much as the "Big Tickets" segment, they still contribute substantially to sales volume.
   
3. **Rare Purchasers**: People in this group make purchases, but less frequently. They often make one-off purchases rather than recurring transactions.
   
4. **Beginners**: Customers in this segment are just starting to make purchases but have yet to establish a consistent pattern. They may represent an opportunity for targeted marketing and
"""